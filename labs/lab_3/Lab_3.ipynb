{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Auto-Correlation Function, Cross-Correlation Function, and AR Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant to improve your understanding of the ARMA model and the correlation functions we've been discussing in class.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to generate a time series drawn from a white noise process and an AR process.\n",
    "2. Be able to calculate the auto-correlation function and interpret the partial auto-correlation function.\n",
    "3. Be able to conduct a Bayesian MLE estimate of AR process parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, uniform\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I:  Auto-Correlation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by implementing the auto-correlation function. __You should implement the equation from scratch using only simple numpy function like sum / mean / etc. Do not use a library that calculates the covariance or correlation function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf_impl(x_t: np.ndarray, n_lags: int) -> np.ndarray:\n",
    "    \"\"\"Calculate the estimated ACF function for a time series.\n",
    "\n",
    "    Args:\n",
    "        x_t: Time series to calculate ACF on.\n",
    "        n_lags: Number of time steps back in time to calculate the ACF on.\n",
    "\n",
    "    Returns:\n",
    "        The estimated ACF for the time series, with the first value being the \n",
    "        correlation function (ACF(0)).\n",
    "    \"\"\"\n",
    "    # TODO: write your code here.\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our function on white noise an make sure we get the right result.\n",
    "\n",
    "$$w_t \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "To do this you will have to:\n",
    "\n",
    "1. Generate white noise with $\\sigma$ = 1 and sample size n = 500 from the process above.\n",
    "2. Calculate the sample ACF up to lag = 20.\n",
    "4. Calculate the analytical ACF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this variable name for the test to work\n",
    "n_lags = 20\n",
    "mean = 0.0\n",
    "std = 1.0\n",
    "n = 500\n",
    "\n",
    "# 1 - Generate realizations from the white noise process.\n",
    "w_t = #TODO\n",
    "\n",
    "# 2 - Calculate the sample ACF with lag of 20 (you already did the hard work).\n",
    "acf_estim_yours = # TODO\n",
    "\n",
    "# 3 - Calculate the analytical ACF. This shouldn't be too many lines of code.\n",
    "acf_analytical = # TODO\n",
    "\n",
    "# Let's check your implementation\n",
    "acf_estim_statsm = acf(w_t, nlags=n_lags)\n",
    "np.testing.assert_array_almost_equal(acf_estim_statsm, acf_estim_yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sit back and let the plotting functions visualize the different quantities.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), dpi=200)\n",
    "fontsize = 15\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "\n",
    "# Plot our white noise.\n",
    "ax1.plot(w_t, label=r'$W_t$', color=colors[0])\n",
    "ax1.set_title('Data', fontsize=fontsize)\n",
    "ax1.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax1.set_ylabel('Signal', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot comparison between analytic and estiamted.\n",
    "ax2.stem(acf_estim_yours, linefmt=colors[0], markerfmt=colors[0], basefmt='k', label='Estimated ACF')\n",
    "ax2.stem(acf_analytical, linefmt=colors[1], markerfmt=colors[1], basefmt='k', label='Analytic ACF')\n",
    "ax2.set_ylim([1.3*min(acf_estim_yours), 1.1*(std**2)])\n",
    "ax2.set_title('ACF Comparison', fontsize=fontsize)\n",
    "ax2.set_xlabel(r'$|h|$', fontsize=fontsize)\n",
    "ax2.set_ylabel(r'$\\rho(|h|)$', fontsize=fontsize)\n",
    "ax2.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot comparison between our estimate and the library estimate.\n",
    "acf_val_impl = acf_impl(x_t=w_t, n_lags=n_lags)\n",
    "ax3.plot(acf_estim_statsm, 'o', color='grey', label='Package ACF')\n",
    "ax3.plot(acf_estim_yours, 'x', ms=6, mew=3, color=colors[0], label='Your ACF')\n",
    "ax3.legend(fontsize=fontsize)\n",
    "ax3.set_xlabel(r'$|h|$', fontsize=fontsize)\n",
    "ax3.set_ylabel(r'$\\rho(|h|)$', fontsize=fontsize)\n",
    "ax3.set_title('Implementation Comparison', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that instead of having a time series with 500 data points, we only had 50. How would you expect the estimated ACF to change? Write you answer below (you won't be judged on the correctness of this answer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've registered your guess, let's try it out. Repeat the steps from above:\n",
    "\n",
    "1. Generate white noise with $\\sigma$ = 1 and sample size n = 50 from the process.\n",
    "2. Calculate the sample ACF up to lag = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this variable name for the test to work\n",
    "n_lags = 20\n",
    "mean = 0.0\n",
    "std = 1.0\n",
    "n = 50\n",
    "\n",
    "# 1 - Generate realizations from the white noise process.\n",
    "w_t_short = #TODO\n",
    "\n",
    "# 2 - Calculate the sample ACF with lag of 20 (you already did the hard work).\n",
    "acf_estim_yours = #TODO\n",
    "\n",
    "# Let's check your implementation again.\n",
    "acf_estim_statsm = acf(w_t_short, nlags=n_lags)\n",
    "np.testing.assert_array_almost_equal(acf_estim_statsm, acf_estim_yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sit back and let the plotting functions visualize the different quantities.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), dpi=100)\n",
    "fontsize = 15\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "\n",
    "# Plot our white noise.\n",
    "ax1.plot(w_t_short, label=r'$W_t$', color=colors[0])\n",
    "ax1.set_title('Data', fontsize=fontsize)\n",
    "ax1.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax1.set_ylabel('Signal', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot comparison between analytic and estimated.\n",
    "ax2.stem(acf_estim_yours, linefmt=colors[0], markerfmt=colors[0], basefmt='k', label='Estimated ACF')\n",
    "ax2.stem(acf_analytical, linefmt=colors[1], markerfmt=colors[1], basefmt='k', label='Analytic ACF')\n",
    "ax2.set_ylim([1.3*min(acf_estim_yours), 1.1])\n",
    "ax2.set_title('ACF Comparison', fontsize=fontsize)\n",
    "ax2.set_xlabel(r'$|h|$', fontsize=fontsize)\n",
    "ax2.set_ylabel(r'$\\rho(|h|)$', fontsize=fontsize)\n",
    "ax2.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening in the plot above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Auto-Correlation Function for AR(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto-correlation function of white noise is somewhat trivial, what about a more interesting process? Let's take an AR(2) process of the form:\n",
    "\n",
    "$$X_{t} = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + W_t$$\n",
    "\n",
    "We can generate the same plots as before, but we will have to:\n",
    "\n",
    "1. Generate samples for the AR(2) process. Assume the boundary condition that $X_0 = W_0$ and $X_1 = \\phi_1 X_0 + W_1$. Also assume that $W_t$ is the same white noise process from above, $\\phi_1 = 0.5$, and $\\phi_2 = 0.2$. Create a sample size of n=500.\n",
    "2. Calculate the sample ACF up to lag = 20.\n",
    "3. You don't need to calculate the analytic ACF. We will save that for the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_two(phi_one: float, phi_two: float, n_samps: int, sigma_w: Optional[float] = 1) -> np.ndarray:\n",
    "    \"\"\"Generate samples from an AR(2) process.\n",
    "\n",
    "    Args:\n",
    "        phi_one: First constant of the AR(2) process.\n",
    "        phi_two: Second constant of the AR(2) process.\n",
    "        n_samps: Number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "        Samples from the AR(2) process.\n",
    "\n",
    "    Notes:\n",
    "        Assumes X_0 = W_0, X_1 = phi_1 X_0 + W_1, and that W_t ~ N(0,1)\n",
    "    \"\"\"\n",
    "    # TODO: write your code here.\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your function against a reference value:\n",
    "np.random.seed(1)\n",
    "test_implementation = ar_two(0.1, 0.1, 5)\n",
    "reference_value = np.array([1.62434536, -0.44932188, -0.4106694, -1.15896775, 0.70844391])\n",
    "np.testing.assert_array_almost_equal(test_implementation, reference_value, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 20\n",
    "np.random.seed(2)\n",
    "\n",
    "# 1. Generate the samples using the function above.\n",
    "x_t = # TODO\n",
    "\n",
    "# 2. Calculate the sample ACF.\n",
    "acf_estim_yours = # TODO\n",
    "\n",
    "# Now sit back and let the plotting functions visualize the different quantities.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), dpi=100)\n",
    "fontsize = 15\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "\n",
    "# Plot our white noise.\n",
    "ax1.plot(w_t, label=r'$W_t$', color=colors[0]) # This is not the same white noise realization as was used for X_t, it's just here for illustration.\n",
    "ax1.plot(x_t, label=r'$X_t$', color=colors[1])\n",
    "ax1.set_title('Data', fontsize=fontsize)\n",
    "ax1.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax1.set_ylabel('Signal', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot comparison between analytic and estiamted.\n",
    "ax2.stem(acf_estim_yours, linefmt=colors[0], markerfmt=colors[0], basefmt='k', label='Estimated ACF')\n",
    "ax2.set_ylim([-0.2, 1.1])\n",
    "ax2.set_title('ACF Estimate', fontsize=fontsize)\n",
    "ax2.set_xlabel(r'$|h|$', fontsize=fontsize)\n",
    "ax2.set_ylabel(r'$\\rho(|h|)$', fontsize=fontsize)\n",
    "ax2.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Partial Auto-Correlation Function for AR(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's clear that the auto-correlation function is more complex for AR(2) than for white noise, it would be hard for us to identify this as an AR(2) process from the auto-correlation function alone. As we discussed in lecture, we can instead use the partial auto-correlation function to achieve this. Implementing the PACF is more involved than what is appropriate for this lab, but we can use the library implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 20\n",
    "\n",
    "pacf_estim = pacf(x_t, n_lags)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), dpi=100)\n",
    "fontsize = 15\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "\n",
    "# Plot our white noise.\n",
    "ax1.plot(w_t, label=r'$W_t$', color=colors[0]) # This is not the same white noise realization as was used for X_t, it's just here for illustration.\n",
    "ax1.plot(x_t, label=r'$X_t$', color=colors[1])\n",
    "ax1.set_title('Data', fontsize=fontsize)\n",
    "ax1.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax1.set_ylabel('Signal', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot comparison between analytic and estiamted.\n",
    "ax2.stem(pacf_estim, linefmt=colors[0], markerfmt=colors[0], basefmt='k', label='Estimated PACF')\n",
    "ax2.set_ylim([-0.2, 1.1])\n",
    "ax2.set_title('PACF Estimate', fontsize=fontsize)\n",
    "ax2.set_xlabel(r'$|h|$', fontsize=fontsize)\n",
    "ax2.set_ylabel(r'$\\rho(|h|)$', fontsize=fontsize)\n",
    "ax2.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about this PACF plot suggests that the process is AR(2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Modeling an AR(2) Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above help us understand what type of model would be approriate for our data. Of course, in practice, we know that this data is AR(2). The next question is how to model the data and use that model to make predictions about future time steps. In the section you will:\n",
    "\n",
    "1. Implement the posterior functions for an AR(2) process given a prior on the parameters.\n",
    "2. Use the posterior fuction to fit the Maximum a posteriori estimate of the AR(2) process parameters.\n",
    "3. Visualize how well those parameters predict future data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goal will be to conduct Bayesian inference. Remember from Lecture 2 that in Bayesian inference we have one quantity of interest, _the posterior of the parameters given the data_, and to calculate it we will need _the prior on the parameters_ and _the likelihood of the data_. The _marginal of the data_ also shows up in our equation, but it is a normalizing factor that does not depend on the parameters of interest. When we are trying to find the maximum a posteriori estimate of our parameter values we can ignore it.\n",
    "\n",
    "First we'll need to specify prior for our AR(2) parameters. There are three free parameters to our version of the model, $\\phi_1$, $\\phi_2$, and $\\sigma_w$. We know that large values of $\\phi_1$ and $\\phi_2$ are unlikely (they will violate causality), so we will set a Gaussian prior for both of them of the form:\n",
    "$$p(\\phi_1) = p(\\phi_2) = \\mathcal{N}(0 , \\sigma_\\phi^2).$$\n",
    "For the standard deviation of the white noise, we have less intuition, but we do know that it must be positive. Therefore, a reasonable distribution that captures our prior understanding is a uniform distribution that is bounded below by 0:\n",
    "$$p(\\sigma_w) = \\mathrm{Unif}[0,\\sigma_\\mathrm{max}].$$\n",
    "1. __Implement a prior function in the class below that return the log prior:__\n",
    "\n",
    " $$\\log p(\\phi_1, \\phi_2, \\sigma_w) = \\log[p(\\phi_1) p(\\phi_2) p(\\sigma_w)] = \\log[p(\\phi_1)] + \\log[p(\\phi_2)] + \\log[p(\\sigma_w)].$$\n",
    "\n",
    "Next, we'll need to define the likelihood of the data given a specific set of parameters. We reviewed this equation in class for an ARMA process. Remember the boundary conditions of our problem.\n",
    "\n",
    "2. __Implement the log likelihood function of the data given the parameters.__\n",
    "\n",
    "Finally we need to implement the posterior given the prior and the likelihood. This should be simple, but if you're confused about the equation refer to the lecture notes.\n",
    "\n",
    "3. __Implement the log posterior function of the parameters given the data.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARTwoModel:\n",
    "    \"\"\"Class implementing prior, likelihood, posterior, and predictions for an AR(2) model.\n",
    "\n",
    "    Args:\n",
    "        sigma_phi: Sigma for the prior on parameters phi_1 and phi_2.\n",
    "        sigma_max: Maximum value for uniform prior on sigma_w.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self: Any, sigma_phi: float, sigma_max: float):\n",
    "        \"\"\"Initialization funciton. See class docstring for parameters.\"\"\"\n",
    "        self.sigma_phi = sigma_phi\n",
    "        self.sigma_max = sigma_max\n",
    "\n",
    "    def log_prior(self: Any, params: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the log prior of the parameters.\n",
    "\n",
    "        Args:\n",
    "            params: Parameters in the order [phi_1, phi_2, sigma_w]\n",
    "\n",
    "        Returns:\n",
    "            Log prior of the parameters.\n",
    "\n",
    "        Notes:\n",
    "            You can use the norm and uniform functions to evaluate the log pdf. These functions come from scipy.stats.\n",
    "        \"\"\"\n",
    "        # TODO: write your code here.\n",
    "        return\n",
    "\n",
    "    def log_likelihood(self: Any, data: np.ndarray, params: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the log likelihood of the data given the parameters.\n",
    "\n",
    "        Args:\n",
    "            data: Observed time series.\n",
    "            params: Parameters in the order [phi_1, phi_2, sigma_w]\n",
    "\n",
    "        Returns:\n",
    "            Log likelihood of the data given the parameters.\n",
    "\n",
    "        Notes:\n",
    "            You may want to vectorize this function as much as possible to improve the speed of your code.\n",
    "            This is not a requirement, just a suggestion.\n",
    "        \"\"\"\n",
    "        # TODO: write your code here.\n",
    "        return\n",
    "\n",
    "    def log_posterior(self: Any, params: np.ndarray, data: np.ndarray) -> float:\n",
    "        \"\"\"Calcualte the log posterior of the parameters given the data.\n",
    "\n",
    "        Args:\n",
    "            params: Parameters in the order [phi_1, phi_2, sigma_w]\n",
    "            data: Observed time series.\n",
    "\n",
    "        Returns:\n",
    "            Log posterior of the parameters given the data.\n",
    "\n",
    "        Notes:\n",
    "            Ignore the marginal of the data (the overall normalization of the posterior).\n",
    "        \"\"\"\n",
    "        # TODO: write your code here.\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run some tests to make sure the implementations are correct.\n",
    "a_model = ARTwoModel(2.0, 5.0)\n",
    "\n",
    "phi_one, phi_two = 0.7, 0.2\n",
    "sigma_w = 2.0\n",
    "params_test = np.array([phi_one, phi_two, sigma_w])\n",
    "x_t = np.array([3.52810469, 3.2699877, 4.9520883, 8.60224575, 10.74710566, 7.28886735, 9.15180511, 7.56132263, 6.91684916, 7.17525595])\n",
    "\n",
    "np.testing.assert_almost_equal(a_model.log_prior([0.3, 0.4, 2.0]), -4.864859339963337)\n",
    "np.testing.assert_almost_equal(a_model.log_prior([0.3, 0.4, 6.0]), -np.inf)\n",
    "np.testing.assert_almost_equal(a_model.log_likelihood(x_t, params_test), -23.520459141643073)\n",
    "np.testing.assert_almost_equal(a_model.log_posterior(params_test, x_t), -28.420318481606408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our posterior in hand we can now use it to extract our maximum a posteriori estimate. We will use a numerical optimizer from scipy. We will test the posteriors we get as a function of two axes:\n",
    "1. The choice of prior. In reality you would only ever select one prior, but this will give us intuition into how important the prior can be in Bayesian inference. Our prior choices will be $\\sigma_\\mathrm{max} = 50.0, \\sigma_\\phi = 20.0$ (a wide prior) and $\\sigma_\\mathrm{max} = 5.0, \\sigma_\\phi = 0.2$ (a narrow prior).\n",
    "2. The length of the data. We will simulate 10000 datapoint, but we will compare the results for subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with out first prior. Let's make it fairly broad in both parameters.\n",
    "sigma_phi = 20.0\n",
    "sigma_max = 50.0\n",
    "a_model_wide = ARTwoModel(sigma_phi, sigma_max)\n",
    "\n",
    "# Generate our data\n",
    "n_samps_data = 10000\n",
    "phi_one, phi_two = 0.7, 0.2\n",
    "sigma_w = 0.3\n",
    "np.random.seed(4)\n",
    "x_t = ar_two(phi_one, phi_two, n_samps_data, sigma_w)\n",
    "\n",
    "# We want the maximum but have a minimizer. So we will just minimize the negative of the log posterior (often called the\n",
    "# negative log posterior). Start with an initial guess of zero for the phis and 1 for sigma_w.\n",
    "params_zero = np.array([0.0, 0.0, 1.0])\n",
    "data_lengths = [5, 25, 125, 625, 3125, 10000]\n",
    "results_wide = []\n",
    "for data_length in data_lengths:\n",
    "    res = minimize(lambda params: -a_model_wide.log_posterior(params, x_t[:data_length]), params_zero, method='nelder-mead', \n",
    "                   options={'xatol': 1e-8, 'disp': False})\n",
    "    results_wide.append(res.x)\n",
    "results_wide = np.array(results_wide)\n",
    "\n",
    "# Lets repeat the process for our narrow prior\n",
    "sigma_phi = 0.2\n",
    "sigma_max = 50.0\n",
    "a_model_narrow = ARTwoModel(sigma_phi, sigma_max)\n",
    "params_zero = np.array([0.0, 0.0, 1.0])\n",
    "results_narrow = []\n",
    "for data_length in data_lengths:\n",
    "    res = minimize(lambda params: -a_model_narrow.log_posterior(params, x_t[:data_length]), params_zero, method='nelder-mead', \n",
    "                   options={'xatol': 1e-8, 'disp': False, 'maxiter': 10000})\n",
    "    results_narrow.append(res.x)\n",
    "results_narrow = np.array(results_narrow)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5), dpi=100)\n",
    "fontsize = 15\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "\n",
    "# Plot our results\n",
    "ax1.plot(data_lengths, results_wide[:,0], label='Wide Prior', color=colors[0])\n",
    "ax1.plot(data_lengths, results_narrow[:,0], label='Narrow Prior', color=colors[1])\n",
    "ax1.set_xscale('log')\n",
    "ax1.axhline(phi_one, c='k', label='True Value')\n",
    "ax1.set_title(r'$\\phi_1$', fontsize=fontsize)\n",
    "ax1.set_xlabel(r'Size of Dataset', fontsize=fontsize)\n",
    "ax1.set_ylabel(r'$\\phi_1$', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot our results\n",
    "ax2.plot(data_lengths, results_wide[:,1], label='Wide Prior', color=colors[0])\n",
    "ax2.plot(data_lengths, results_narrow[:,1], label='Narrow Prior', color=colors[1])\n",
    "ax2.set_xscale('log')\n",
    "ax2.axhline(phi_two, c='k', label='True Value')\n",
    "ax2.set_title(r'$\\phi_2$', fontsize=fontsize)\n",
    "ax2.set_xlabel(r'Size of Dataset', fontsize=fontsize)\n",
    "ax2.set_ylabel(r'$\\phi_2$', fontsize=fontsize)\n",
    "ax2.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot our results\n",
    "ax3.plot(data_lengths, results_wide[:,2], label='Wide Prior', color=colors[0])\n",
    "ax3.plot(data_lengths, results_narrow[:,2], label='Narrow Prior', color=colors[1])\n",
    "ax3.set_xscale('log')\n",
    "ax3.axhline(sigma_w, c='k', label='True Value')\n",
    "ax3.set_title(r'$\\sigma_w$', fontsize=fontsize)\n",
    "ax3.set_xlabel(r'Size of Dataset', fontsize=fontsize)\n",
    "ax3.set_ylabel(r'$\\sigma_w$', fontsize=fontsize)\n",
    "ax3.legend(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does our choice of prior matter? Where does it matter the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our fit model for prediction! Our distribution of interest is:\n",
    "$$p(X_{t+1:t+n}|X_{1:t},\\phi_1,\\phi_2,\\sigma_w).$$\n",
    "To visualize our predictions, we can either sample from this distribution or we can plot its mean and variance. The mean and variance of an AR(2) process is a bit involved, so here we'll just focus on drawing samples. You will:\n",
    "1. Write code to sample from the conditional distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_x_future(data: np.ndarray, params: np.ndarray, n_future: int) -> np.ndarray:\n",
    "    \"\"\"Sample from the posterior of X_{t+1:t+n} given X_{1:t} and the parameters of our AR(2) process.\n",
    "\n",
    "    Args:\n",
    "        data: Observed time series up to time t.\n",
    "        params: Parameters in the order [phi_1, phi_2, sigma_w]\n",
    "        n_future: How many steps in the future to sample.\n",
    "\n",
    "    Returns:\n",
    "        Sample of X_{t+1:t+n}\n",
    "    \"\"\"\n",
    "    # TODO: write your code here. \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the predictions being output by our model\n",
    "n_future = 100\n",
    "n_samples_posterior = 5000\n",
    "samples = np.zeros((n_samples_posterior, n_future))\n",
    "for i in range(n_samples_posterior):\n",
    "    # Use the full data prediction with the narrow prior as our parameters.\n",
    "    samples[i] = sample_x_future(x_t, results_narrow[-1], n_future)\n",
    "\n",
    "t_data = np.arange(n_samps_data)\n",
    "t_samples = np.arange(n_samps_data, n_samps_data + n_future)\n",
    "fig = plt.figure(figsize=(8,8), dpi=100)\n",
    "plt.plot(t_data, x_t, color=colors[1], label='Data')\n",
    "plt.plot(t_samples, np.mean(samples, axis=0), color=colors[0], label='Sample Mean')\n",
    "plt.fill_between(t_samples, np.mean(samples, axis=0) - np.std(samples, axis=0), np.mean(samples, axis=0) + np.std(samples, axis=0), \n",
    "                color=colors[0], label='Sample Standard Deviation', alpha=0.2)\n",
    "plt.xlim([n_samps_data-200, n_samps_data + n_future])\n",
    "plt.ylabel(r'$X_t$', fontsize=fontsize)\n",
    "plt.xlabel('Time Index', fontsize=fontsize)\n",
    "plt.title('Posterior Sampling', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the mean behave the way that it does? What about the standard deviation of the sample? How does this relate to the auto-correlation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "66320cd066d3355eb7ffdb51393be04b9cd51411977f0c18905a000f1686306a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

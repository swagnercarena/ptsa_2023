{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA Modeling Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview__: This lab is meant to further improve your understanding of the ARMA model and the correlation functions we've been discussing in class. To get you more comfortable with the type of work you will need to do for the project, this will be less on-the-rails than the previous lab. You should take inspiration from the previous labs to help organize your code.\n",
    "\n",
    "__Goals__: Students should:\n",
    "\n",
    "1. Be able to identify a good ARMA model to apply on a dataset.\n",
    "2. Be able to fit an ARMA model to the data.\n",
    "3. Be able to display the conditional posterior coming from an ARMA process fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, uniform\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_process import arma_acovf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Identifying the Best ARMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start from a pre-generated dataset for this lab. We believe that this data comes from a stationary process, and we would like to model the data as ARMA(p,q). The question is what the correct value of p and q are for this process. Using the ACF and PACF (you are free to use the implementations from statsmodels that are imported above) make an argument for why this data corresponds to an AR(3) process. This should include:\n",
    "\n",
    "* Calculating the ACF, PACF, and expected error on both estimates (use the formulas from class).\n",
    "* Plotting the data, the ACF, and the PACF along with the errors. This should be three seperate plots, one for the data, one for the ACF, and one for the PACF.\n",
    "* Labeling all of the plots. This includes axes, sensible colors, and legends for each of your plots.\n",
    "\n",
    "_Remember, the previous labs have copious examples of how to make nice plots. I would suggest using plt.subplots, fill_between, and stem._\n",
    "\n",
    "__You will be graded on the quality of the plot you produce, not just the correctness of the information being presented.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Read the signal from the provided file. Be sure to convert to a numpy array!\n",
    "df = # TODO\n",
    "signal = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the ACF and PACF along with the expected error for the estimators.\n",
    "n_lags = 20\n",
    "acf_estimate = # TODO\n",
    "pacf_estimate = # TODO\n",
    "acf_error_estimate = # TODO\n",
    "pacf_error_estimate = # TODO\n",
    "\n",
    "# TODO: Do the plotting here. See the previous labs for examples.\n",
    "raise ValueError('There should be some plots here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make your argument in this cell based on the plots above. Convert this cell to markdown.\n",
    "\n",
    "raise ValueError('Make your argument.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Fitting our AR(3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've hopefully made a reasonable argument that fitting the parameters of an AR(3) model will give us a good result. Now it's time to do that fit. To do this you will have to:\n",
    "\n",
    "* Calculating the posterior for the parameters of an AR(3) process.\n",
    "* Calculate the conditional distribution for an AR(3) process given its parameters.\n",
    "* Plot samples and the conditional distirbution for our AR(3) process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARThreeModel:\n",
    "    \"\"\"Class implementing prior, likelihood, posterior, and predictions for an AR(3) model.\n",
    "\n",
    "    Args:\n",
    "        sigma_phi: Sigma for the prior on parameters phi_1, phi_2, and phi_3.\n",
    "        sigma_max: Maximum value for uniform prior on sigma_w.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self: Any, sigma_phi: float, sigma_max: float):\n",
    "        \"\"\"Initialization funciton. See class docstring for parameters.\"\"\"\n",
    "        self.sigma_phi = sigma_phi\n",
    "        self.sigma_max = sigma_max\n",
    "\n",
    "    def log_prior(self: Any, params: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the log prior of the parameters.\n",
    "\n",
    "        Args:\n",
    "            params: Parameters in the order [phi_1, phi_2, phi_3, sigma_w]\n",
    "\n",
    "        Returns:\n",
    "            Log prior of the parameters.\n",
    "\n",
    "        Notes:\n",
    "            You can use the norm and uniform functions to evaluate the log pdf. These functions come from scipy.stats.\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        return\n",
    "\n",
    "    def log_likelihood(self: Any, data: np.ndarray, params: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the log likelihood of the data given the parameters.\n",
    "\n",
    "        Args:\n",
    "            data: Observed time series.\n",
    "            params: Parameters in the order [phi_1, phi_2, phi_3, sigma_w]\n",
    "\n",
    "        Returns:\n",
    "            Log likelihood of the data given the parameters.\n",
    "\n",
    "        Notes:\n",
    "            You may want to vectorize this function as much as possible to improve the speed of your code.\n",
    "            This is not a requirement, just a suggestion.\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        return\n",
    "\n",
    "    def log_posterior(self: Any, params: np.ndarray, data: np.ndarray) -> float:\n",
    "        \"\"\"Calcualte the log posterior of the parameters given the data.\n",
    "\n",
    "        Args:\n",
    "            params: Parameters in the order [phi_1, phi_2, sigma_w]\n",
    "            data: Observed time series.\n",
    "\n",
    "        Returns:\n",
    "            Log posterior of the parameters given the data.\n",
    "\n",
    "        Notes:\n",
    "            Ignore the marginal of the data (the overall normalization of the posterior).\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run some tests to make sure the implementations are correct.\n",
    "a_model = ARThreeModel(2.0, 5.0)\n",
    "\n",
    "phi_one, phi_two, phi_three = 0.5, 0.2, 0.1\n",
    "sigma_w = 1.0\n",
    "params_test = np.array([phi_one, phi_two, phi_three, sigma_w])\n",
    "x_t = np.array([3.52810469, 3.2699877, 4.9520883, 8.60224575, 10.74710566, 7.28886735, 9.15180511, 7.56132263, 6.91684916, 7.17525595])\n",
    "\n",
    "np.testing.assert_almost_equal(a_model.log_prior([0.3, 0.4, 0.5, 2.0]), -6.508195053727955)\n",
    "np.testing.assert_almost_equal(a_model.log_prior([0.3, 0.4, 0.5, 6.0]), -np.inf)\n",
    "np.testing.assert_almost_equal(a_model.log_likelihood(x_t, params_test), -50.47684010239573)\n",
    "np.testing.assert_almost_equal(a_model.log_posterior(params_test, x_t), -56.96003515612368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit our model to the data using the priors.\n",
    "sigma_phi = 2.0\n",
    "sigma_max = 5.0\n",
    "a_model_wide = ARThreeModel(sigma_phi, sigma_max)\n",
    "# TODO run the fit with minimize from scipy. Results will be [phi_1, phi_2, phi_3, sigma_w].\n",
    "results = # TODO: Run the fit. See last week's lab for an example.\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'phi_{i} has MAP value of {results[i]}')\n",
    "print(f'sigma_w has MAP value of {results[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the conditional distribution of our AR(3) process. As we discussed in class, we can take advantage of the fact that all of the structure of our process can be captured by a multivariate Gaussian distribution. To use our conditional Gaussian equation from lecture we need a mean and a covariance matrix. The mean is easy, but we will need a function for the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_three_cov_matrix(phis: np.ndarray, max_h: int, sigma_w: float) -> np.ndarray:\n",
    "    \"\"\"Calculate the covariance matrix for an AR(3) process.\n",
    "\n",
    "    Args:\n",
    "        phis: Phi parameters of the AR(3) process.\n",
    "        max_h: Maximum time index seperation to calculate the covariance matrix for.\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix up to lag of max_h.\n",
    "\n",
    "    Notes:\n",
    "        Use the function arma_acovf to calculate the . It takes as input ([phi_0, -phi_one, ...], [theta_0, theta_one, ...], max_h + 1, \n",
    "        sigma_w ** 2). Note that it requires a value of phi_0 and theta_0 (which we always take to be 1.0).\n",
    "    \"\"\"\n",
    "    # TODO: Implement.\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly test you got the right answer.\n",
    "cov_test = ar_three_cov_matrix(np.array([0.5, 0.2, 0.1]), 1, 1.5)\n",
    "cov_correct = np.array([[4.795507, 3.369816], [3.369816, 4.795507]])\n",
    "np.testing.assert_array_almost_equal(cov_test, cov_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the covariance matrix for your fit parameters and a max_h of 20.\n",
    "cov_twenty = # TODO: Calculate and plot. I recommend plt.imshow(). Labels are less important here, but a colorbar is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the structure make sense? How would it be different if it was an MA(3) process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this cell to markdown and write your answer to the question above.\n",
    "raise ValueError('Answer the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the conditional distribution for the next m time steps given the past n. The equation can be found in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditioned_mean_covariance(mu_vec: np.ndarray, covariance_matrix: np.ndarray, \n",
    "                                conditioned_observations: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the conditioned mean and covariance matrix of a multivariate Gaussian.\n",
    "\n",
    "    Args:\n",
    "        mu_vec: Mean of distribution with length (n+m).\n",
    "        covariance_matrix: Covariance matrix of the distribution with shape (n+m * n+m).\n",
    "        conditioned_observations: First n observations that are being conditioned on.\n",
    "\n",
    "    Returns:\n",
    "        Mean and covariance of the next m observations.\n",
    "\n",
    "    Notes:\n",
    "        The code from Lab 2 may be useful, but notice that the calculation here is slightly different.\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the conditioned mean and covariance.\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conditioned mean and covariance we can now better understand our model's fit. We want to do two things here:\n",
    "\n",
    "1. Do one-step-ahead prediction using the previous 40 time steps to confirm that our model gives a reasonable fit to the data.\n",
    "2. Predict the next 70 timesteps given the last 40 time steps in our signal (i.e. predict the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Want to test the function you wrote above? The sample_x_future function from last week might be useful!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our mean and covariances for the one ahead.\n",
    "one_ahead_start = len(signal) - 100\n",
    "mu_one_ahead = np.zeros(len(signal) - one_ahead_start)\n",
    "cov_one_ahead = np.zeros(len(signal) - one_ahead_start) # Covariance matrix is a single value for one-ahead prediction.\n",
    "\n",
    "n_lookback = 40\n",
    "for time_step in range(one_ahead_start, len(signal)):\n",
    "    # TODO: Use your function to calculate the covariance matrix for the time step in question using the last\n",
    "    # 40 datapoints of signal.\n",
    "    mu_joint = # TODO\n",
    "    cov_joint = # TODO\n",
    "    mu_cond, cov_cond = # TODO\n",
    "    \n",
    "    mu_one_ahead[time_step - one_ahead_start] = mu_cond[0]\n",
    "    cov_one_ahead[time_step - one_ahead_start] = cov_cond[0,0]\n",
    "\n",
    "# TODO: Predict the next 70 timesteps given the last 40.\n",
    "predict_next = 70\n",
    "n_lookback = 40\n",
    "\n",
    "mu_joint = # TODO\n",
    "cov_joint = # TODO\n",
    "mu_future, cov_future = # TODO\n",
    "\n",
    "mu_total = np.concatenate([mu_one_ahead, mu_future])\n",
    "var_total = np.concatenate([cov_one_ahead, np.diag(cov_future)])\n",
    "\n",
    "# Plot the result\n",
    "colors = ['#7fcdbb', '#2c7fb8']\n",
    "t_data = np.arange(len(signal))\n",
    "t_cond = np.arange(one_ahead_start, len(signal))\n",
    "t_total = np.arange(one_ahead_start, len(signal) + predict_next)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), dpi=200)\n",
    "ax1.plot(t_data, signal, color=colors[1], label='Data')\n",
    "ax1.plot(t_total, mu_total, color=colors[0])\n",
    "ax1.fill_between(t_total, mu_total - 2 * np.sqrt(var_total), mu_total + 2 * np.sqrt(var_total), \n",
    "                 color=colors[0], label=r'Conditional Distribution', alpha=0.4)\n",
    "\n",
    "ax1.set_xlim([one_ahead_start, len(signal) + predict_next + 2])\n",
    "ax1.set_ylabel(r'$X_t$', fontsize=fontsize)\n",
    "ax1.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax1.set_title('Posterior Predictions', fontsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "\n",
    "# Plot the residual.\n",
    "ax2.plot(t_cond, signal[one_ahead_start:] - mu_one_ahead, color=colors[1], label='Data')\n",
    "ax2.plot(t_cond, np.zeros(len(t_cond)), color=colors[0])\n",
    "ax2.fill_between(t_cond,  -2 * np.sqrt(cov_one_ahead), 2 * np.sqrt(cov_one_ahead), \n",
    "                 color=colors[0], label=r'Conditional Distribution', alpha=0.4)\n",
    "\n",
    "ax2.set_xlim([one_ahead_start, len(signal) + 2])\n",
    "ax2.set_ylabel(r'Residual', fontsize=fontsize)\n",
    "ax2.set_xlabel('Time Index', fontsize=fontsize)\n",
    "ax2.set_title('Posterior Residuals', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked, almost all of your residuals should fall within the conditional distribution contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
